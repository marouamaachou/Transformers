{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHUw_vdPVAN4",
        "colab_type": "code",
        "outputId": "bc924fad-b4e3-48c9-d353-208cf91659e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pylab import rcParams\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import math\n",
        "from tensorflow.keras import optimizers, backend\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, AlphaDropout, Layer, Embedding, Attention, LayerNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzxvFa8w_BX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "  d_k = query.get_shape()[-1]\n",
        "  scores = tf.matmul(query, key, transpose_b=True) / 8\n",
        "  print(scores.shape)\n",
        "  weights = tf.nn.softmax(scores)\n",
        "  print(weights.shape)\n",
        "  s_atten = tf.matmul(weights, value)\n",
        "  print(s_atten.shape)\n",
        "\n",
        "  if dropout is not None:\n",
        "      s_attn = dropout(s_attn)\n",
        "  \n",
        "  return s_atten, weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG4Nkud9YTy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(Layer):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.l_1 = Dense(d_ff, activation='relu')\n",
        "        self.l_2 = Dense(d_model)\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.dropout(self.l_2(self.l_1(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHe3PgKClUgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "\n",
        "  def __init__(self, h, d_model, dropout=0.1):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    assert d_model % h == 0\n",
        "    self.d_k = d_model // h\n",
        "    self.h = h\n",
        "    #self.linears = Dense(d_model, d_model)\n",
        "    self.attn = None\n",
        "    self.dropout = Dropout(dropout)\n",
        "    \n",
        "    self.wq = Dense(d_model)\n",
        "    self.wk = Dense(d_model)\n",
        "    self.wv = Dense(d_model)\n",
        "\n",
        "    self.dense = Dense(d_model)\n",
        "\n",
        "  def attention_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, self.h, -1, self.d_k))\n",
        "    return x\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)\n",
        "    k = self.wk(k)\n",
        "    v = self.wv(v)\n",
        "    print(q.shape, k.shape, v.shape)\n",
        "    \n",
        "    q = self.attention_heads(q, batch_size)\n",
        "    k = self.attention_heads(k, batch_size)  \n",
        "    v = self.attention_heads(v, batch_size)\n",
        "    print(q.shape, k.shape, v.shape)\n",
        "\n",
        "    attn, weights = attention(q, k, v, mask)\n",
        "    attn = tf.transpose(attn, perm=[0, 2, 1, 3])\n",
        "    concat_attention = tf.reshape(attn, (batch_size, -1, self.h * self.d_k))\n",
        "    output = self.dense(concat_attention)\n",
        "    print('output', output.shape)\n",
        "    return output, weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OF2IbdFbWBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class PositionalEncoding(Layer):\n",
        "  def __init__(self, d_model, rate=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        self.pe = np.zeros((max_len, d_model))\n",
        "\n",
        "        position = np.arange(0, max_len, dtype='float32')\n",
        "        position = np.expand_dims(position, axis=1)\n",
        "\n",
        "        div_term = np.exp(np.arange(0, d_model, 2, dtype='float32') * (- np.log(10000.0) / d_model))\n",
        "\n",
        "        self.pe[:, 0::2] = np.sin(position * div_term)\n",
        "        self.pe[:, 1::2] = np.cos(position * div_term)\n",
        "        \n",
        "        self.pe=np.expand_dims(self.pe,axis=0)\n",
        "        self.pe=tf.cast(self.pe, dtype=tf.float32)\n",
        "\n",
        "        self.dropout = Dropout(rate)\n",
        "\n",
        "  def call(self, x):\n",
        "        x = x + tf.Variable(self.pe[:, :x.get_shape()[1]], \n",
        "                            trainable=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1BFP2z2cA4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(Layer): \n",
        "    def __init__(self,d_model):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = Dense(d_model)\n",
        "        \n",
        "    def call(self, x):\n",
        "        return tf.nn.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpGRjHxVWk6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(Layer) : \n",
        "\n",
        "  def __init__(self, d_model, h, output_dim, rate=0.1,**kwargs):\n",
        "    self.attention = MultiHeadAttention(h, d_model)\n",
        "    self.Norm1 = LayerNormalization(axis=-1,epsilon=1e-6)\n",
        "    self.feedforward = FeedForward(d_model,output_dim)\n",
        "    self.Norm2 = LayerNormalization(axis=-1,epsilon=1e-6)\n",
        "    self.dk = np.sqrt(64)\n",
        "    self.output_dim = output_dim\n",
        "    self.dropout1 = Dropout(rate)\n",
        "    self.dropout2 = Dropout(rate)\n",
        "    super(EncoderLayer, self).__init__(**kwargs)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    attn_output, _ = self.attention(x, x, x, mask)\n",
        "    attn_output = self.dropout1(attn_output)\n",
        "    print(attn_output.shape, x.shape)\n",
        "    output1 = self.Norm1(x + attn_output)\n",
        "    \n",
        "    ff_output = self.feedforward(output1)\n",
        "    ff_output = self.dropout2(ff_output)\n",
        "    print(ff_output.shape, output1.shape)\n",
        "    output2 = self.Norm2(output1 + ff_output)\n",
        "\n",
        "    return output2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlVe1kGVY5i_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(Layer):\n",
        "  \n",
        "    def __init__(self, d_model, h, dff, rate=0.1,**kwargs):\n",
        "      self.attention = MultiHeadAttention(h, d_model)\n",
        "      self.Norm1 = LayerNormalization(axis=-1, epsilon=1e-6)\n",
        "      self.attention_mask = MultiHeadAttention(h, d_model)\n",
        "      self.Norm2 = LayerNormalization(axis=-1, epsilon=1e-6)\n",
        "      self.feedforward = FeedForward(d_model,dff)\n",
        "      self.Norm3 = LayerNormalization(axis=-19, epsilon=1e-6)\n",
        "      self.dropout1 = Dropout(rate)\n",
        "      self.dropout2 = Dropout(rate)\n",
        "      self.dropout3 = Dropout(rate)\n",
        "      super(DecoderLayer, self).__init__(**kwargs)\n",
        "\n",
        "        \n",
        "    def call(self, x, enc_x, training, mask, padding_mask):\n",
        "      attn_output, weights1 = self.attention(x, x, x, mask) \n",
        "      attn_output = self.dropout1(attn_output)\n",
        "      output1 = self.Norm1(x + attn_output)\n",
        "      attn_output2, weights2 = self.attention(enc_x, enc_x, output1 , padding_mask) \n",
        "      attn_output2 = self.dropout2(attn_output2)\n",
        "      output2 = self.Norm2(output1 + attn_output2)\n",
        "      output3 = self.dropout2(self.feedforward(output2))\n",
        "      output3 = self.Norm2(output2 + output3)\n",
        "      return output3, weights1, weights2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEAGbgmVhA-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(Model):\n",
        "  def __init__ (self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    \n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.enc_Embedding = Embedding(input_vocab_size, d_model)\n",
        "    self.pos_enc = PositionalEncoding(d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dec_Embedding = Embedding(target_vocab_size, d_model)    \n",
        "    self.pos_dec = PositionalEncoding(d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.generator = Generator(d_model)\n",
        "    #self.dropout = Dropout(rate)\n",
        "\n",
        "  def call(self, enc_x, dec_x, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    print(enc_x.shape)\n",
        "    enc_x = self.enc_Embedding(enc_x)\n",
        "    print(enc_x.shape)\n",
        "    enc_x = self.pos_enc(enc_x)\n",
        "    print(enc_x.shape)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      print(i)\n",
        "      enc_x = self.enc_layers[i](enc_x, training, enc_padding_mask)\n",
        "      print(enc_x.shape)\n",
        "    \n",
        "    enc_output = enc_x\n",
        "\n",
        "    print(dec_x.shape)\n",
        "    dec_x = self.dec_Embedding(dec_x)\n",
        "    print(dec_x.shape)\n",
        "    dec_x = self.pos_dec(dec_x)\n",
        "    print(dec_x.shape)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      dec_output, block1, block2 = self.dec_layers[i](dec_x, enc_output, training,\n",
        "                                                      look_ahead_mask, dec_padding_mask)\n",
        "      print(dec_output.shape)\n",
        "      #attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      #attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "    output = self.generator(dec_output)\n",
        "\n",
        "    #return output, attention_weights\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR3-F5zPcj1B",
        "colab_type": "code",
        "outputId": "b3a6ff09-3be1-4805-abe4-29a9f953b986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "model = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500, target_vocab_size=8500, \n",
        "    pe_input=10000, pe_target=6000,rate=0.1)\n",
        "\n",
        "x_input = tf.random.uniform((32, 20), dtype=tf.int64, minval=0, maxval=8500)\n",
        "x_target = tf.random.uniform((32, 20), dtype=tf.int64, minval=0, maxval=8500)\n",
        "\n",
        "output = model(x_input,\n",
        "               x_target,\n",
        "               training=False,\n",
        "               enc_padding_mask=None,\n",
        "               look_ahead_mask=None,\n",
        "               dec_padding_mask=None)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 20)\n",
            "(32, 20, 512)\n",
            "(32, 20, 512)\n",
            "0\n",
            "(32, 20, 512) (32, 20, 512) (32, 20, 512)\n",
            "(32, 8, 20, 64) (32, 8, 20, 64) (32, 8, 20, 64)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 64)\n",
            "output (32, 20, 512)\n",
            "(32, 20, 512) (32, 20, 512)\n",
            "(32, 20, 512) (32, 20, 512)\n",
            "(32, 20, 512)\n",
            "1\n",
            "(32, 20, 512) (32, 20, 512) (32, 20, 512)\n",
            "(32, 8, 20, 64) (32, 8, 20, 64) (32, 8, 20, 64)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 64)\n",
            "output (32, 20, 512)\n",
            "(32, 20, 512) (32, 20, 512)\n",
            "(32, 20, 512) (32, 20, 512)\n",
            "(32, 20, 512)\n",
            "(32, 20)\n",
            "(32, 20, 512)\n",
            "(32, 20, 512)\n",
            "(32, 20, 512) (32, 20, 512) (32, 20, 512)\n",
            "(32, 8, 20, 64) (32, 8, 20, 64) (32, 8, 20, 64)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 64)\n",
            "output (32, 20, 512)\n",
            "(32, 20, 512) (32, 20, 512) (32, 20, 512)\n",
            "(32, 8, 20, 64) (32, 8, 20, 64) (32, 8, 20, 64)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 64)\n",
            "output (32, 20, 512)\n",
            "(32, 20, 512)\n",
            "(32, 20, 512) (32, 20, 512) (32, 20, 512)\n",
            "(32, 8, 20, 64) (32, 8, 20, 64) (32, 8, 20, 64)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 64)\n",
            "output (32, 20, 512)\n",
            "(32, 20, 512) (32, 20, 512) (32, 20, 512)\n",
            "(32, 8, 20, 64) (32, 8, 20, 64) (32, 8, 20, 64)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 20)\n",
            "(32, 8, 20, 64)\n",
            "output (32, 20, 512)\n",
            "(32, 20, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlDgB1I-z161",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "f5ceee93-5aa7-4e51-bd7d-d340a38eebd4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      multiple                  4352000   \n",
            "_________________________________________________________________\n",
            "positional_encoding_4 (Posit multiple                  0         \n",
            "_________________________________________________________________\n",
            "encoder_layer_4 (EncoderLaye multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "encoder_layer_5 (EncoderLaye multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      multiple                  4352000   \n",
            "_________________________________________________________________\n",
            "positional_encoding_5 (Posit multiple                  0         \n",
            "_________________________________________________________________\n",
            "decoder_layer_4 (DecoderLaye multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "decoder_layer_5 (DecoderLaye multiple                  3152384   \n",
            "_________________________________________________________________\n",
            "generator_2 (Generator)      multiple                  262656    \n",
            "=================================================================\n",
            "Total params: 21,576,192\n",
            "Trainable params: 21,576,192\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE77bHuBWSXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}